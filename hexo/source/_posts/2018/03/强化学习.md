---
title: 强化学习
date: 2018-03-02
tags: [强化学习]
categories: A1深度学习
---

@(A1深度学习)[强化学习]

整理强化学习学习笔记
- - -
<!--more-->
强化学习
---

# 强化学习的定义
机器学习一共有三个分支，有监督学习、无监督学习和强化学习。强化学习是系统从环境学习以使得奖励最大的机器学习。
强化学习是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。
其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。

在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。
在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。
在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。

# 强化学习出现背景

# MDP(Markov Decision Process )马尔可夫决策过程
## 马尔科夫性
马尔可夫性质是概率论中的一个概念。
当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；
换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有马尔可夫性质。
## 马尔科夫链
马尔可夫链是满足马尔可夫性质的离散事件随机过程，该过程中，在给定当前知识或信息的情况下，过去（即当前以前的历史状态）对于预测将来（即当前以后的未来状态）是无关的。
## 马尔科夫决策过程
马尔可夫决策过程是基于马尔可夫过程理论的随机动态系统的决策过程，其分五个部分：
1. $S$表示状态集 (states)；
2.  $A$表示动作集 (Action)；
3.  $P_{s,a}^{s^‘}$表示状态 $s$ 下采取动作$a$之后转移到 $s^‘$状态的概率；
4.  $R_{s,a}$表示状态$s$下采取动作$a$获得的奖励；
5.  $\gamma$是衰减因子。

和一般的马尔科夫过程不同，马尔科夫决策过程考虑了`动作`，即系统`下个状态`不仅和`当前的状态`有关，也和`当前采取的动作`有关。
>举下棋的例子，当我们在某个局面（状态s）走了一步 (动作 a )。这时对手的选择（导致下个状态 s’）我们是不能确定的，但是他的选择只和 s 和 a 有关，而不用考虑更早之前的状态和动作，即 s’是根据 s 和 a 随机生成的。

## 策略和价值

## 状态估值函数
## 动作估值函数

## 最优策略存在性和贝尔曼等式
贝尔曼等式表明了当前状态的值函数与下个状态的值函数的关系，具有很简明的形式。

# 强化学习算法
马尔科夫决策过程是强化学习的理论基础。不管我们是将强化学习应用于五子棋游戏、星际争霸还是机器人行走，我们都假设背后存在了一个马尔科夫决策过程。
只不过有的时候我们知道马尔科夫决策过程所有信息(状态集合，动作集合，转移概率和奖励)，有的时候我们只知道部分信息 (状态集合和动作集合)，还有些时候马尔科夫决策过程的信息太大无法全部存储 (比如围棋的状态集合总数为)。
## 基于模型 (Model-based) 
基于模型的强化学习算法是知道并可以存储所有马尔科夫决策过程信息。
### 动态规划方法
## 非基于模型 (Model-free)
非基于模型的强化学习算法则需要自己探索未知的马尔科夫过程。
### 蒙特卡洛树搜索方法
### TD时间差分方法
### Q-Learning方法

# 强化学习在量化投资领域的应用
# 常见问题
## 强化学习与监督学习的区别
强化学习和有监督学习的不同在于教师信号。强化学习的教师信号是动作的奖励，有监督学习的教师信号是正确的动作。
## 与深度学习神经网络算法的关系

# 参考
强化学习系列教程：http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html
莫烦强化学习系列教程：https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/
这里有一篇深度强化学习劝退文：https://zhuanlan.zhihu.com/p/33936457
algorithmdog强化学习系列教程：http://www.algorithmdog.com/ml/rl-series


