<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"geosmart.github.io","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="dolphinscheduler的调度任务有spark，默认仅支持spark on yarn,本文记录了折腾spark on k8s的踩坑过程，主要难点是镜像的制作，kerberos认证和hive的兼容">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkOnK8S踩坑记录">
<meta property="og:url" content="http://geosmart.github.io/2021/11/21/SparkOnK8S%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="geosmart.io">
<meta property="og:description" content="dolphinscheduler的调度任务有spark，默认仅支持spark on yarn,本文记录了折腾spark on k8s的踩坑过程，主要难点是镜像的制作，kerberos认证和hive的兼容">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://geosmart.github.io/2021/11/21/SparkOnK8S%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/sparkonk8s.png">
<meta property="og:image" content="https://spark.apache.org/docs/3.1.2/img/k8s-cluster-mode.png">
<meta property="article:published_time" content="2021-11-21T01:44:05.000Z">
<meta property="article:modified_time" content="2021-12-15T01:13:13.054Z">
<meta property="article:author" content="geosmart">
<meta property="article:tag" content="dolphinScheduler">
<meta property="article:tag" content="k8s">
<meta property="article:tag" content="cdh">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://geosmart.github.io/2021/11/21/SparkOnK8S%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/sparkonk8s.png">

<link rel="canonical" href="http://geosmart.github.io/2021/11/21/SparkOnK8S%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>SparkOnK8S踩坑记录 | geosmart.io</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">geosmart.io</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">勿助勿忘，深造自得</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/geosmart" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://geosmart.github.io/2021/11/21/SparkOnK8S%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="geosmart">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="geosmart.io">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparkOnK8S踩坑记录
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：21   十一月   2021 9:44:05" itemprop="dateCreated datePublished" datetime="2021-11-21T09:44:05+08:00">21   十一月   2021</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：15   十二月   2021 9:13:13" itemprop="dateModified" datetime="2021-12-15T09:13:13+08:00">15   十二月   2021</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>dolphinscheduler的调度任务有spark，默认仅支持spark on yarn,<br>本文记录了折腾spark on k8s的踩坑过程，主要难点是镜像的制作，kerberos认证和hive的兼容</p>
<a id="more"></a>
<p><img src="sparkonk8s.png" alt="spark-on-k8s"></p>
<h1 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h1><h2 id="部署要求"><a href="#部署要求" class="headerlink" title="部署要求"></a>部署要求</h2><ul>
<li>实验版Spark 2.3+，正式版Spark 3.0+</li>
<li>Kubernetes 1.6+</li>
<li>具有Kubernetes pods的list, create, edit和delete权限</li>
<li>Kubernetes集群必须正确配置Kubernetes DNS</li>
</ul>
<h2 id="验证环境"><a href="#验证环境" class="headerlink" title="验证环境"></a>验证环境</h2><p>cdh版本:hadoop2.6.0-cdh5.16.1 </p>
<ul>
<li>hive：1.1.0</li>
<li>hadoop：２.6.0</li>
<li>spark：2.4.8/3.1.2</li>
</ul>
<h1 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h1><p>当我们通过<code>spark-submit</code>将Spark作业提交到Kubernetes集群时，会执行以下流程：<br><img src="https://spark.apache.org/docs/3.1.2/img/k8s-cluster-mode.png" alt="k8s-cluster-mode"></p>
<ul>
<li>Spark在<code>Kubernetes pod</code>中创建Spark <code>driver</code>;</li>
<li>Driver调用Kubernetes API创建<code>executor</code> pods，executor pods执行作业代码;</li>
<li>计算作业结束，<code>executor</code> pods回收并清理;</li>
<li><code>driver</code> pod处于completed状态，保留日志，直到Kubernetes GC或者手动清理;</li>
</ul>
<h1 id="制作docker镜像"><a href="#制作docker镜像" class="headerlink" title="制作docker镜像"></a>制作docker镜像</h1><p>由于cdh的包封装了很多再用的组件，所以先制作一个<code>cdh-base</code>的镜像。<br>后续还有<code>dolphinscheduler-worker</code>,<code>flink</code>,<code>spark</code>的镜像都基于<code>cdh-base</code>制作。</p>
<h2 id="cdh-base的镜像"><a href="#cdh-base的镜像" class="headerlink" title="cdh-base的镜像"></a>cdh-base的镜像</h2><p>坑：cdh的parcel包官方已不提供下载了,可以从网上找到一些别人下载分享的。<br>下载好后，上传到内部网盘,然后制作docker镜像并push到内部harbor仓库.<br><a href="cdh-base-dockerfile">Dockerfile</a><br><a href="cdh-base-clear.sh">clear.sh</a></p>
<h2 id="spark镜像"><a href="#spark镜像" class="headerlink" title="spark镜像"></a>spark镜像</h2><p>下载spark3.1.2的官方压缩包，里面自带dockerfile，主要修改基于cdh包制作</p>
<ul>
<li><a href="spark-dockerfile">Dockerfile</a></li>
<li><a href="spark-entrypoint.sh">entrypoint.sh</a></li>
</ul>
<p>entrypoint修改内容：</p>
<ul>
<li>根据环境变量<code>SPARK_VERSION</code>设置的spark版本设置<code>SPARK_HOME</code></li>
<li>加载Hosts</li>
<li>新增jars_ext到<code>SPARK_DIST_CLASSPATH</code></li>
<li>外部可通过<code>PYSPARK_PYTHON</code>设置pyspark的虚拟环境</li>
</ul>
<h1 id="配置K8S环境"><a href="#配置K8S环境" class="headerlink" title="配置K8S环境"></a>配置K8S环境</h1><p>需要运维新增namespace和service account和pvc</p>
<h2 id="Spark的PVC配置"><a href="#Spark的PVC配置" class="headerlink" title="Spark的PVC配置"></a>Spark的PVC配置</h2><h3 id="spark-pvc"><a href="#spark-pvc" class="headerlink" title="spark pvc"></a>spark pvc</h3><ul>
<li>spark-eventlog：spark执行事件日志</li>
<li>spark-config：spark和hdfs/minio/hive等服务的配置文件，host文件路径</li>
<li>spark-application：spark执行所需的资源,如jar,py,keytab等</li>
</ul>
<h3 id="mount-glusterfs-pvc"><a href="#mount-glusterfs-pvc" class="headerlink" title="mount glusterfs pvc"></a>mount glusterfs pvc</h3><p>spark-config<br><code>mount -t glusterfs -o backup-volfile-servers=glusterfs_ip  k8s_ip:spark-config /var/lib/dolphinscheduler/worker-data/spark-config</code></p>
<p>spark-application<br><code>mount -t glusterfs -o backup-volfile-servers=glusterfs_ip  k8s_ip:spark-application /var/lib/dolphinscheduler/worker-data/exec/process</code></p>
<p>spark-submit提交客户端需要mount spark的pvc的资源，保持简单（客户端的路径和pod内的路径一致）</p>
<h1 id="采集executor日志"><a href="#采集executor日志" class="headerlink" title="采集executor日志"></a>采集executor日志</h1><p><code>Spark On Yarn</code>，可以开启<code>yarn.log-aggregation-enable</code>将日志收集聚合到HDFS中，以供查看。<br><code>Spark On Kubernetes</code>，则缺少这种日志收集机制，我们只能通过Kubernetes pod的日志输出，来查看Spark的日志：</p>
<p>这个在k8s运维层解决即可，k8s通过fluent-bit会将pod日志都采集到es.</p>
<h1 id="访问driver-ui"><a href="#访问driver-ui" class="headerlink" title="访问driver ui"></a>访问driver ui</h1><ul>
<li>官方提供的port-forward方案，<code>kubectl -n spark port-forward podName 4000:4040</code>，这种比较原始，每个spark-submit都需要启动一个代理服务，适合测试，不适合生产环境</li>
<li><p>生产环境应类似部署独立的service和ingress提供外部访问,注意需要配置<code>spark.ui.proxyRedirectUri</code>参数为ingress的path</p>
</li>
<li><p><a href="spark-ui-ingress">spark-ui-ingress.yaml</a></p>
</li>
<li><a href="spark-ui-svc">spark-ui-svc.yaml</a></li>
</ul>
<h1 id="部署history-server"><a href="#部署history-server" class="headerlink" title="部署history-server"></a>部署history-server</h1><ol>
<li>spark程序开启eventlog，写入挂载的eventlog的pvc目录</li>
<li>部署<code>spark historyserver</code>查看，挂载eventlog的pvc目录</li>
</ol>
<h1 id="spark数据存储"><a href="#spark数据存储" class="headerlink" title="spark数据存储"></a>spark数据存储</h1><ul>
<li>s3a(minio):spark状态数据</li>
<li>hdfs：业务数据</li>
</ul>
<h2 id="S3A"><a href="#S3A" class="headerlink" title="S3A"></a>S3A</h2><p>在<code>$SPARK_HOME</code>内新增目录jars_ext，将扩展jar放入<code>spark-3.1.2-bin-hadoop2.7.tgz</code>对应的aws版本(这个版本不能向下兼容,版本不能搞错)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#&#x2F;usr&#x2F;lib&#x2F;cdh&#x2F;lib&#x2F;spark3&#x2F;jars_ext</span><br><span class="line">├── [ 11M]  aws-java-sdk-1.7.4.jar</span><br><span class="line">└── [123K]  hadoop-aws-2.7.7.jar</span><br></pre></td></tr></table></figure>
<p>然后在entrypoint.sh中加入<br><code>export SPARK_DIST_CLASSPATH=&quot;$SPARK_DIST_CLASSPATH:${SPARK_HOME}/jars_ext/*&quot;</code></p>
<p>spark-submit参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.hadoop.fs.s3a.impl&#x3D;org.apache.hadoop.fs.s3a.S3AFileSystem \</span><br><span class="line">--conf spark.hadoop.fs.s3a.endpoint&#x3D;http:&#x2F;&#x2F;ip:port \</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key&#x3D;xxx \</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key&#x3D;xxx</span><br></pre></td></tr></table></figure>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>hive和hdfs集成涉及</p>
<ul>
<li>python-venv:用于设置PYSPARK_PYTHON</li>
<li>配置文件：hdfs-site.xml,core-site.xml（可包含s3a配置）,hive-site.xml,yarn-site.xml(mapreduce计算依赖此配置)</li>
<li>认证文件：keytab,krb5.conf</li>
<li>数据文件：application提交的jar和py,zip依赖</li>
</ul>
<p>Spark 运行需要配置 Hadoop 的配置文件位置，如果 ClassPath 没有，则无法访问 HDFS<br><code>export HADOOP_CONF_DIR=/opt/spark/conf</code></p>
<p>hdfs测试脚本: <code>hdfs dfs -copyFromLocal people.json hdfs://ha/user/hive/examples/src/main/resources/people.json</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;run-example  \</span><br><span class="line">--conf spark.sql.hive.metastore.version&#x3D;1.1  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars&#x3D;path \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path&#x3D;file:&#x2F;&#x2F;usr&#x2F;lib&#x2F;cdh&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;*.jar \</span><br><span class="line">--verbose \</span><br><span class="line">sql.JavaSparkSQLExample</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$&#123;SPARK3_HOME&#125;&#x2F;bin&#x2F;run-example --verbose  \</span><br><span class="line">--master local \</span><br><span class="line">--conf spark.driver.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot;  \</span><br><span class="line">--conf spark.executor.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot;  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path&#x3D;file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;*.jar,file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hadoop&#x2F;client&#x2F;*.jar  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars&#x3D;path  \</span><br><span class="line">--conf spark.sql.hive.metastore.version&#x3D;1.1  \</span><br><span class="line">--conf spark.kerberos.principal&#x3D;x  \</span><br><span class="line">--conf spark.kerberos.keytab&#x3D;&#x2F;dboard&#x2F;application&#x2F;2&#x2F;1963&#x2F;22621&#x2F;39887&#x2F;x.keytab  \</span><br><span class="line">--conf spark.files.fetchTimeout&#x3D;5m  \</span><br><span class="line">--conf spark.files.overwrite&#x3D;true  \</span><br><span class="line">--conf spark.driver.memory&#x3D;1048M  \</span><br><span class="line">--conf spark.executor.memory&#x3D;1024M  \</span><br><span class="line">--conf spark.executor.instances&#x3D;2  \</span><br><span class="line"> sql.JavaSparkSQLExample</span><br></pre></td></tr></table></figure>
<p>local测试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$&#123;SPARK3_HOME&#125;&#x2F;bin&#x2F;spark-submit --verbose  \</span><br><span class="line">--master local \</span><br><span class="line">--conf spark.driver.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot;  \</span><br><span class="line">--conf spark.executor.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot;  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path&#x3D;file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;*.jar,file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hadoop&#x2F;client&#x2F;*.jar  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars&#x3D;path  \</span><br><span class="line">--conf spark.sql.hive.metastore.version&#x3D;1.1  \</span><br><span class="line">--conf spark.kerberos.principal&#x3D;x  \</span><br><span class="line">--conf spark.kerberos.keytab&#x3D;&#x2F;dboard&#x2F;application&#x2F;2&#x2F;1963&#x2F;22621&#x2F;39887&#x2F;x.keytab  \</span><br><span class="line">--conf spark.files.fetchTimeout&#x3D;5m  \</span><br><span class="line">--conf spark.files.overwrite&#x3D;true  \</span><br><span class="line">--conf spark.driver.memory&#x3D;1048M  \</span><br><span class="line">--conf spark.executor.memory&#x3D;1024M  \</span><br><span class="line">--conf spark.executor.instances&#x3D;2  \</span><br><span class="line"> demo.py</span><br></pre></td></tr></table></figure></p>
<p>k8s测试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">export KUBECONFIG&#x3D;&#x2F;opt&#x2F;dolphinscheduler&#x2F;conf&#x2F;config&#x2F;kube_config.yaml</span><br><span class="line">$&#123;SPARK3_HOME&#125;&#x2F;bin&#x2F;spark-submit  --verbose  \</span><br><span class="line">--conf spark.driver.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot;  \</span><br><span class="line">--conf spark.executor.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot;  --master k8s:&#x2F;&#x2F;https:&#x2F;&#x2F;ip:6443 --deploy-mode cluster  \</span><br><span class="line">--conf spark.kubernetes.namespace&#x3D;dboard  \</span><br><span class="line">--conf spark.kubernetes.container.image&#x3D;harbor.dc.xxx-it.com&#x2F;x-bigdata&#x2F;spark:latest  \</span><br><span class="line">--conf spark.kubernetes.authenticate.driver.serviceAccountName&#x3D;dboard  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.SPARK_VERSION&#x3D;3  \</span><br><span class="line">--conf spark.executorEnv.SPARK_VERSION&#x3D;3  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path&#x3D;file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;*.jar,file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hadoop&#x2F;client&#x2F;*.jar  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars&#x3D;path  \</span><br><span class="line">--conf spark.sql.hive.metastore.version&#x3D;1.1  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-log.options.claimName&#x3D;dboard-log  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-log.mount.path&#x3D;&#x2F;dboard&#x2F;log&#x2F;spark-eventlog  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-log.mount.readOnly&#x3D;false  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-log.mount.subPath&#x3D;spark-eventlog  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-log.options.claimName&#x3D;dboard-log  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-log.mount.path&#x3D;&#x2F;dboard&#x2F;log&#x2F;spark-eventlog  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-log.mount.readOnly&#x3D;false  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-log.mount.subPath&#x3D;spark-eventlog  \</span><br><span class="line">--conf spark.eventLog.dir&#x3D;&#x2F;dboard&#x2F;log&#x2F;spark-eventlog  \</span><br><span class="line">--conf spark.eventLog.enabled&#x3D;true  \</span><br><span class="line">--conf spark.eventLog.compress&#x3D;true  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-config.options.claimName&#x3D;dboard-config  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-config.mount.path&#x3D;&#x2F;dboard&#x2F;config  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-config.mount.readOnly&#x3D;true  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-config.options.claimName&#x3D;dboard-config  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-config.mount.path&#x3D;&#x2F;dboard&#x2F;config  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-config.mount.readOnly&#x3D;true  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.HADOOP_CONF_DIR&#x3D;&#x2F;dboard&#x2F;config&#x2F;hadoop  \</span><br><span class="line">--conf spark.executorEnv.HADOOP_CONF_DIR&#x3D;&#x2F;dboard&#x2F;config&#x2F;hadoop  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.PYSPARK_PYTHON&#x3D;&#x2F;dboard&#x2F;config&#x2F;python-venv&#x2F;bin&#x2F;python  \</span><br><span class="line">--conf spark.executorEnv.PYSPARK_PYTHON&#x3D;&#x2F;dboard&#x2F;config&#x2F;python-venv&#x2F;bin&#x2F;python  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.HOSTS&#x3D;&#x2F;dboard&#x2F;config&#x2F;hosts  \</span><br><span class="line">--conf spark.executorEnv.HOSTS&#x3D;&#x2F;dboard&#x2F;config&#x2F;hosts  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-application.options.claimName&#x3D;dboard-application  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-application.mount.path&#x3D;&#x2F;dboard&#x2F;application  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.dboard-application.mount.readOnly&#x3D;false  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-application.options.claimName&#x3D;dboard-application  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-application.mount.path&#x3D;&#x2F;dboard&#x2F;application  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.dboard-application.mount.readOnly&#x3D;false  \</span><br><span class="line">--conf spark.kubernetes.kerberos.krb5.path&#x3D;&#x2F;etc&#x2F;krb5.conf  \</span><br><span class="line">--conf spark.kerberos.principal&#x3D;x  \</span><br><span class="line">--conf spark.kerberos.keytab&#x3D;&#x2F;dboard&#x2F;application&#x2F;2&#x2F;1963&#x2F;22621&#x2F;39887&#x2F;x.keytab  \</span><br><span class="line">--conf spark.kubernetes.file.upload.path&#x3D;s3a:&#x2F;dboard&#x2F;__SPARK_APP__  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.endpoint&#x3D;http:&#x2F;&#x2F;ip:32030  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key&#x3D;DYaDwXsj8VRtWYPSbr7A  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key&#x3D;z7HAEhdyseNX9AVyzDLAJzEjZChJsnAf1f7VehE  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.impl&#x3D;org.apache.hadoop.fs.s3a.S3AFileSystem  \</span><br><span class="line">--conf spark.files.fetchTimeout&#x3D;5m  \</span><br><span class="line">--conf spark.files.overwrite&#x3D;true  \</span><br><span class="line">--conf spark.driver.memory&#x3D;2048M  \</span><br><span class="line">--conf spark.executor.memory&#x3D;1024M  \</span><br><span class="line">--conf spark.executor.instances&#x3D;2  \</span><br><span class="line">--conf spark.kubernetes.executor.request.cores&#x3D;2  \</span><br><span class="line">--conf spark.kubernetes.driver.request.cores&#x3D;1  \</span><br><span class="line">--conf spark.kubernetes.driver.pod.name&#x3D;spark-demo1    local:&#x2F;&#x2F;&#x2F;dboard&#x2F;application&#x2F;2&#x2F;1963&#x2F;22621&#x2F;39887&#x2F;demo.py</span><br></pre></td></tr></table></figure></p>
<h1 id="kerberos认证"><a href="#kerberos认证" class="headerlink" title="kerberos认证"></a>kerberos认证</h1><h2 id="手动kinit"><a href="#手动kinit" class="headerlink" title="手动kinit"></a>手动kinit</h2><p>在spark２.4.8尝试<br>自行编译spark代码集成hadoop２.6,hive-1.1.0</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">./dev/make-distribution.sh \</span><br><span class="line">--name 2.6.0-cdh5.16.1 \</span><br><span class="line">--tgz \</span><br><span class="line">-Phadoop-2.6 \</span><br><span class="line">-Dhadoop.version=2.6.0-cdh5.16.1 \</span><br><span class="line">-Phive \</span><br><span class="line">-Phive-thriftserver \</span><br><span class="line">-Pyarn \</span><br><span class="line">-Pkubernetes \</span><br><span class="line">-Pexternal-jars \</span><br><span class="line">-DskipTests</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># entrypoint.sh do kerberos auth by environment variable</span></span><br><span class="line">rm -f /etc/krb5.conf</span><br><span class="line">ln -s <span class="variable">$&#123;KRB5_CONFIG&#125;</span> /etc/krb5.conf</span><br><span class="line">kinit -kt <span class="variable">$&#123;KEYTAB&#125;</span> <span class="variable">$PRINCIPAL</span></span><br><span class="line"><span class="comment"># scheduly update krb5 tgt</span></span><br><span class="line">crontab -l | &#123; cat; <span class="built_in">echo</span> <span class="string">"0 */10 * * *  kinit -kt <span class="variable">$&#123;KEYTAB&#125;</span> <span class="variable">$PRINCIPAL</span>"</span>; &#125; | crontab -</span><br><span class="line"></span><br><span class="line">spark-submit xxx</span><br></pre></td></tr></table></figure>
<blockquote>
<p>kerberos认证问题<br>通过spark.executorEnv传入环境变量(krb5.conf、keytab、principal)到Node，完成kerberos认证。<br>Pod内测试pyspark是能够正常执行hdfs和hive的操作；<br>pyspark用master(‘local’)模式代码也工作正常；<br>但是spark-submit的python代码在k8s集群的executor上spark.read操作hdfs文件报kerberos认证异常。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">Job aborted due to stage failure: Task <span class="number">1</span> in stage <span class="number">1.0</span> failed <span class="number">4</span> times, most recent failure: Lost task <span class="number">1.3</span> in stage <span class="number">1.0</span> (TID <span class="number">5</span>, ip, executor <span class="number">2</span>): </span><br><span class="line">java.io.IOException: </span><br><span class="line">Failed on local exception: </span><br><span class="line">java.io.IOException: </span><br><span class="line">org.apache.hadoop.security.AccessControlException: </span><br><span class="line">Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : </span><br><span class="line">local host is: <span class="string">"pysparkapp-1631176344099-exec-2/ip"</span>; </span><br><span class="line">destination host is: <span class="string">"ip"</span>:<span class="number">8020</span>; </span><br><span class="line">trueat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:<span class="number">772</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1508</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1441</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:<span class="number">231</span>)</span><br><span class="line">trueat com.sun.proxy.$Proxy20.create(Unknown Source)</span><br><span class="line">trueat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:<span class="number">313</span>)</span><br><span class="line">trueat sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)</span><br><span class="line">trueat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:<span class="number">43</span>)</span><br><span class="line">trueat java.lang.reflect.Method.invoke(Method.java:<span class="number">498</span>)</span><br><span class="line">trueat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:<span class="number">258</span>)</span><br><span class="line">trueat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:<span class="number">104</span>)</span><br><span class="line">trueat com.sun.proxy.$Proxy21.create(Unknown Source)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:<span class="number">2146</span>)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<span class="number">1804</span>)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:<span class="number">1728</span>)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DistributedFileSystem$<span class="number">7</span>.doCall(DistributedFileSystem.java:<span class="number">438</span>)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DistributedFileSystem$<span class="number">7</span>.doCall(DistributedFileSystem.java:<span class="number">434</span>)</span><br><span class="line">trueat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:<span class="number">81</span>)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<span class="number">434</span>)</span><br><span class="line">trueat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:<span class="number">375</span>)</span><br><span class="line">trueat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<span class="number">926</span>)</span><br><span class="line">trueat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:<span class="number">907</span>)</span><br><span class="line">trueat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:<span class="number">74</span>)</span><br><span class="line">trueat org.apache.parquet.hadoop.ParquetFileWriter.&lt;init&gt;(ParquetFileWriter.java:<span class="number">248</span>)</span><br><span class="line">trueat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:<span class="number">390</span>)</span><br><span class="line">trueat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:<span class="number">349</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.&lt;init&gt;(ParquetOutputWriter.scala:<span class="number">37</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$<span class="number">1</span>.newInstance(ParquetFileFormat.scala:<span class="number">151</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:<span class="number">120</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.&lt;init&gt;(FileFormatDataWriter.scala:<span class="number">108</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:<span class="number">240</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$<span class="number">1</span>.apply(FileFormatWriter.scala:<span class="number">174</span>)</span><br><span class="line">trueat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$<span class="number">1</span>.apply(FileFormatWriter.scala:<span class="number">173</span>)</span><br><span class="line">trueat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:<span class="number">90</span>)</span><br><span class="line">trueat org.apache.spark.scheduler.Task.run(Task.scala:<span class="number">123</span>)</span><br><span class="line">trueat org.apache.spark.executor.Executor$TaskRunner$$anonfun$<span class="number">10</span>.apply(Executor.scala:<span class="number">411</span>)</span><br><span class="line">trueat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:<span class="number">1360</span>)</span><br><span class="line">trueat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:<span class="number">417</span>)</span><br><span class="line">trueat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:<span class="number">1149</span>)</span><br><span class="line">trueat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:<span class="number">624</span>)</span><br><span class="line">trueat java.lang.Thread.run(Thread.java:<span class="number">748</span>)</span><br><span class="line">Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection$<span class="number">1</span>.run(Client.java:<span class="number">718</span>)</span><br><span class="line">trueat java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">trueat javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>)</span><br><span class="line">trueat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1924</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:<span class="number">681</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:<span class="number">769</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection.access$<span class="number">3000</span>(Client.java:<span class="number">396</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client.getConnection(Client.java:<span class="number">1557</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client.call(Client.java:<span class="number">1480</span>)</span><br><span class="line">true... <span class="number">39</span> more</span><br><span class="line">Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]</span><br><span class="line">trueat org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:<span class="number">172</span>)</span><br><span class="line">trueat org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:<span class="number">396</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:<span class="number">594</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection.access$<span class="number">2000</span>(Client.java:<span class="number">396</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection$<span class="number">2</span>.run(Client.java:<span class="number">761</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection$<span class="number">2</span>.run(Client.java:<span class="number">757</span>)</span><br><span class="line">trueat java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">trueat javax.security.auth.Subject.doAs(Subject.java:<span class="number">422</span>)</span><br><span class="line">trueat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:<span class="number">1924</span>)</span><br><span class="line">trueat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:<span class="number">756</span>)</span><br><span class="line">true... <span class="number">42</span> more</span><br><span class="line"></span><br><span class="line">Driver stacktrace:</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h2 id="DelegationTokenFetcher"><a href="#DelegationTokenFetcher" class="headerlink" title="DelegationTokenFetcher"></a>DelegationTokenFetcher</h2><blockquote>
<p>按这个方案尝试<br><a href="https://stackoverflow.com/questions/54181560/when-running-spark-on-kubernetes-to-access-kerberized-hadoop-cluster-how-do-you">when-running-spark-on-kubernetes-to-access-kerberized-hadoop-cluster-how-do-you</a></p>
</blockquote>
<p>First get the delegation token from hadoop using the below command .</p>
<ol>
<li>Do a kinit -kt with your keytab and principal</li>
<li>Execute the below to store the hdfs delegation token in a tmp path <code>spark-submit --class org.apache.hadoop.hdfs.tools.DelegationTokenFetcher &quot;&quot; --renewer null /tmp/spark.token</code></li>
<li>Do your actual spark submit with the adding this configuration . <code>--conf spark.executorEnv.HADOOP_TOKEN_FILE_LOCATION=/tmp/spark.token</code>\<br>The above is how yarn executors authenticate. Do the same for kubernetes executors too.<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class org.apache.hadoop.hdfs.tools.DelegationTokenFetcher "" --renewer null /tmp/spark.token</span><br><span class="line">--conf spark.executorEnv.HADOOP_TOKEN_FILE_LOCATION=/tmp/spark.token \</span><br></pre></td></tr></table></figure>
container起来了然后一直在renew也不退出，也不继续执行代码…</li>
</ol>
<p><code>spark on k8s折腾2天，hdfs都好的，但是卡在没解决hive数据源的kerberos认证问题</code></p>
<p>放弃2.4了，升级３.1.2，基于cdh的包，lib下放入spark2和spark3的版本，做成镜像</p>
<h2 id="升级spark３"><a href="#升级spark３" class="headerlink" title="升级spark３"></a>升级spark３</h2><p>kerberos新增３个参数，支持kerberos认证，krb5Conf会写入executor的configMap<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.kubernetes.kerberos.krb5.path&#x3D;&#x2F;spark&#x2F;application&#x2F;2&#x2F;1963&#x2F;20954&#x2F;37472&#x2F;hive_krb5.conf  \</span><br><span class="line">--conf spark.kerberos.principal&#x3D;hive  \</span><br><span class="line">--conf spark.kerberos.keytab&#x3D;&#x2F;spark&#x2F;application&#x2F;2&#x2F;1963&#x2F;20954&#x2F;37472&#x2F;hive.keytab  \</span><br></pre></td></tr></table></figure></p>
<h1 id="spark-hive版本兼容"><a href="#spark-hive版本兼容" class="headerlink" title="spark-hive版本兼容"></a>spark-hive版本兼容</h1><p>之前花了很大力气自己编译spark源码，兼容自己的hadoop和hive版本。</p>
<p>最后发现不需要这么整，Spark，对hive数据源做了很好的抽象，外部制定hive版本和jar包路径，spark内部HiveClient会记载对应的HiveConf和执行支持的Operation。</p>
<p>做了以下尝试：</p>
<ol>
<li>spark3.1.2版本修改源码，hive版本差异太大，尝试了下，错误太多，impossible mission；</li>
<li><code>dev/make-distribution.sh</code>直接修改编译的hadoop版本，编译不通过</li>
<li>在spark运行的时候，动态加载hive对应的版本包</li>
</ol>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">- </span>In Spark 3.0, we upgraded the built-in Hive from 1.2 to 2.3 and it brings following impacts:</span><br><span class="line"></span><br><span class="line"><span class="bullet">  * </span>You may need to set <span class="code">`spark.sql.hive.metastore.version`</span> and <span class="code">`spark.sql.hive.metastore.jars`</span> according to the version of the Hive metastore you want to connect to. </span><br><span class="line"><span class="bullet">  * </span>For example: set <span class="code">`spark.sql.hive.metastore.version`</span> to <span class="code">`1.2.1`</span> and <span class="code">`spark.sql.hive.metastore.jars`</span> to <span class="code">`maven`</span> if your Hive metastore version is 1.2.1.</span><br><span class="line"><span class="bullet">  * </span>You need to migrate your custom SerDes to Hive 2.3 or build your own Spark with <span class="code">`hive-1.2`</span> profile. See [<span class="string">HIVE-15167</span>](<span class="link">https://issues.apache.org/jira/browse/HIVE-15167</span>) for more details.</span><br><span class="line"></span><br><span class="line"><span class="bullet">  * </span>The decimal string representation can be different between Hive 1.2 and Hive 2.3 when using <span class="code">`TRANSFORM`</span> operator in SQL for script transformation, which depends on hive's behavior. In Hive 1.2, the string representation omits trailing zeroes. But in Hive 2.3, it is always padded to 18 digits with trailing zeroes if necessary.</span><br></pre></td></tr></table></figure>
<p>官方上面这段话，说可以自己编译hive-1.2，实际编译不了.<br>spark-3.0.2官方有提供编译的下载包，后续版本没有看到<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;dev&#x2F;make-distribution.sh --name 2.6.0-cdh5.16.1  --tgz  -Phive-1.2 -Phive-thriftserver -Pyarn  -Pkubernetes -Dhadoop.version&#x3D;2.6.0-cdh5.16.1</span><br></pre></td></tr></table></figure><br>这个<code>maven</code>模式，可以设置<code>spark.sql.maven.additionalRemoteRepositories</code>配置自定义maven仓库</p>
<p>根据官网的说明 ,spark从1.4.0 开始就能和不同的hive元数据进行交互，也就是说spark编译的hive内部版本和spark访问hive的元数据是独立的，可以配置不同的hive版本进行对应元数据的访问。</p>
<blockquote>
<p>interacting-with-different-versions-of-hive-metastore</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.sql.catalogImplementation hive</span><br><span class="line">spark.sql.hive.metastore.version 1.1.0</span><br><span class="line">spark.sql.hive.metastore.jars <span class="variable">$&#123;env:HADOOP_COMMON_HOME&#125;</span>/../hive/lib/*:<span class="variable">$&#123;env:HADOOP_COMMON_HOME&#125;</span>/client/*</span><br></pre></td></tr></table></figure>
<p>具体代码在org.apache.spark.sql.hive.HiveUtils和org.apache.spark.sql.hive.client.IsolatedClientLoader中,<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def hiveVersion(version: String): HiveVersion &#x3D; version match &#123;</span><br><span class="line">  case &quot;12&quot; | &quot;0.12&quot; | &quot;0.12.0&quot; &#x3D;&gt; hive.v12</span><br><span class="line">  case &quot;13&quot; | &quot;0.13&quot; | &quot;0.13.0&quot; | &quot;0.13.1&quot; &#x3D;&gt; hive.v13</span><br><span class="line">  case &quot;14&quot; | &quot;0.14&quot; | &quot;0.14.0&quot; &#x3D;&gt; hive.v14</span><br><span class="line">  case &quot;1.0&quot; | &quot;1.0.0&quot; | &quot;1.0.1&quot; &#x3D;&gt; hive.v1_0</span><br><span class="line">  case &quot;1.1&quot; | &quot;1.1.0&quot; | &quot;1.1.1&quot; &#x3D;&gt; hive.v1_1</span><br><span class="line">  case &quot;1.2&quot; | &quot;1.2.0&quot; | &quot;1.2.1&quot; | &quot;1.2.2&quot; &#x3D;&gt; hive.v1_2</span><br><span class="line">  case &quot;2.0&quot; | &quot;2.0.0&quot; | &quot;2.0.1&quot; &#x3D;&gt; hive.v2_0</span><br><span class="line">  case &quot;2.1&quot; | &quot;2.1.0&quot; | &quot;2.1.1&quot; &#x3D;&gt; hive.v2_1</span><br><span class="line">  case &quot;2.2&quot; | &quot;2.2.0&quot; &#x3D;&gt; hive.v2_2</span><br><span class="line">  case &quot;2.3&quot; | &quot;2.3.0&quot; | &quot;2.3.1&quot; | &quot;2.3.2&quot; | &quot;2.3.3&quot; | &quot;2.3.4&quot; | &quot;2.3.5&quot; | &quot;2.3.6&quot; | &quot;2.3.7&quot; &#x3D;&gt;</span><br><span class="line">    hive.v2_3</span><br><span class="line">  case &quot;3.0&quot; | &quot;3.0.0&quot; &#x3D;&gt; hive.v3_0</span><br><span class="line">  case &quot;3.1&quot; | &quot;3.1.0&quot; | &quot;3.1.1&quot; | &quot;3.1.2&quot; &#x3D;&gt; hive.v3_1</span><br><span class="line">  case version &#x3D;&gt;</span><br><span class="line">    throw new UnsupportedOperationException(s&quot;Unsupported Hive Metastore version ($version). &quot; +</span><br><span class="line">      s&quot;Please set $&#123;HiveUtils.HIVE_METASTORE_VERSION.key&#125; with a valid version.&quot;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="测试路径"><a href="#测试路径" class="headerlink" title="测试路径"></a>测试路径</h2><ol>
<li>是基于withouthadoop还是with hadoop的版本？<ul>
<li><code>with hadoop</code>版本，必须是官方的，不能替换为其他hadoop版本。</li>
</ul>
</li>
<li>spark3.1.2的jars目录下的hive　jar是否需要删除？<ul>
<li>不需要，spark中HiveClient会自动加载对应版本的</li>
</ul>
</li>
<li>修改hive版本后，测试是否能加载jar…</li>
</ol>
<h2 id="测试HiveClasspath"><a href="#测试HiveClasspath" class="headerlink" title="测试HiveClasspath"></a>测试HiveClasspath</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/lib/cdh/lib/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(<span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin/hadoop classpath):<span class="variable">$SPARK_HOME</span>/jars </span><br><span class="line"></span><br><span class="line">./bin/spark-shell \</span><br><span class="line">--conf spark.sql.catalogImplementation=hive  \</span><br><span class="line">--conf spark.sql.warehouse.dir=/user/hive/warehouse  \</span><br><span class="line">--verbose \</span><br><span class="line">--conf spark.sql.hive.metastore.version=1.1.0  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars=path \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path=<span class="string">"file:///usr/lib/cdh/lib/hive/lib/*"</span> \</span><br><span class="line">--conf spark.driver.extraJavaOptions=<span class="string">"-Dlog4j.debug"</span> \</span><br><span class="line">--conf spark.executor.extraJavaOptions=<span class="string">"-Dlog4j.debug"</span> \</span><br><span class="line">sql.hive.JavaSparkHiveExample</span><br></pre></td></tr></table></figure>
<p>心里总是不停的问,\为什么就是不加载｀spark.sql.hive.metastore.jars.path｀目录下的jar…</p>
<p>通过以下spark-shell<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-shell \</span><br><span class="line">--verbose \</span><br><span class="line">--conf spark.sql.hive.metastore.version&#x3D;1.1  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars&#x3D;classpath \</span><br><span class="line">--conf spark.driver.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot; \</span><br><span class="line">--conf spark.executor.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot; \</span><br><span class="line">sql.JavaSparkSQLExample</span><br></pre></td></tr></table></figure><br>定位问题用下面的代码救命.执行代码查看classpath<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.lang.ClassLoader</span><br><span class="line">ClassLoader.getSystemClassLoader.asInstanceOf[java.net.URLClassLoader].getURLs.foreach(println)</span><br><span class="line"></span><br><span class="line">println(spark.version)</span><br><span class="line">println(org.apache.hadoop.util.VersionInfo.getVersion)</span><br><span class="line">println(org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion)</span><br></pre></td></tr></table></figure></p>
<h2 id="spark2和spark3与hive集成区别"><a href="#spark2和spark3与hive集成区别" class="headerlink" title="spark2和spark3与hive集成区别"></a>spark2和spark3与hive集成区别</h2><ol>
<li>download <code>spark-3.1.2-bin-hadoop2.7.tgr.gz</code>,and prepare dir with <code>2.6.0-cdh5.16.1</code>, the hive version is <code>1.1.0</code></li>
<li>switch to a new user and set env to recover exist environment variable</li>
</ol>
<blockquote>
<p>主要是<code>spark.sql.hive.metastore.jars</code>不一样，看spark源码才能找到答案</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_CONF_DIR=/opt/cdh/lib/hadoop/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> SPARK2_HOME=/opt/cdh/lib/spark2</span><br><span class="line"><span class="built_in">export</span> SPARK3_HOME=/opt/cdh/lib/spark3</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/tmp/dboard/dboard-config/hadoop</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=</span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=</span><br><span class="line"><span class="comment"># run-example of spark 2:jars path is standard jvm class path , only support absolutely path or end with *</span></span><br><span class="line"><span class="variable">$SPARK2_HOME</span>/bin/run-example \</span><br><span class="line">--master <span class="built_in">local</span> \</span><br><span class="line">--verbose \</span><br><span class="line">--conf spark.sql.hive.metastore.version=1.1 \</span><br><span class="line">--conf spark.sql.hive.metastore.jars=<span class="string">"/opt/cdh/lib/hive/lib/*:/opt/cdh/lib/hadoop/client/*"</span> \</span><br><span class="line">--conf spark.driver.extraJavaOptions=<span class="string">"-Dlog4j.debug"</span> \</span><br><span class="line">--conf spark.executor.extraJavaOptions=<span class="string">"-Dlog4j.debug"</span> \</span><br><span class="line">--conf spark.kubernetes.driverEnv.SPARK_VERSION=2 \</span><br><span class="line">--conf spark.executorEnv.SPARK_VERSION=2 \</span><br><span class="line">sql.hive.JavaSparkHiveExample</span><br><span class="line"></span><br><span class="line"><span class="comment"># run-example of spark3: jars path is better than spark2,join by comma</span></span><br><span class="line"><span class="variable">$SPARK3_HOME</span>/bin/run-example \</span><br><span class="line">--master <span class="built_in">local</span> \</span><br><span class="line">--verbose \</span><br><span class="line">--conf spark.sql.hive.metastore.version=1.1 \</span><br><span class="line">--conf spark.sql.hive.metastore.jars=path \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path=<span class="string">"file:///opt/cdh/lib/hive/lib/*.jar,file:///opt/cdh/lib/hadoop/client/*.jar"</span> \</span><br><span class="line">--conf spark.driver.extraJavaOptions=<span class="string">"-Dlog4j.debug"</span> \</span><br><span class="line">--conf spark.executor.extraJavaOptions=<span class="string">"-Dlog4j.debug"</span> \</span><br><span class="line">--conf spark.kubernetes.driverEnv.SPARK_VERSION=2 \</span><br><span class="line">--conf spark.executorEnv.SPARK_VERSION=2 \</span><br><span class="line">sql.hive.JavaSparkHiveExample</span><br></pre></td></tr></table></figure>
<p>基于<code>spark-3.1.2-bin-hadoop2.7</code>测试通过<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;run-example \</span><br><span class="line">--verbose \</span><br><span class="line">--conf spark.sql.hive.metastore.version&#x3D;1.1.1  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars&#x3D;path \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path&#x3D;&quot;file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hive&#x2F;lib&#x2F;*.jar,file:&#x2F;&#x2F;&#x2F;opt&#x2F;cdh&#x2F;lib&#x2F;hadoop&#x2F;client&#x2F;*.jar&quot; \</span><br><span class="line">--conf spark.driver.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot; \</span><br><span class="line">--conf spark.executor.extraJavaOptions&#x3D;&quot;-Dlog4j.debug&quot; \</span><br><span class="line">sql.hive.JavaSparkHiveExample</span><br></pre></td></tr></table></figure><br>当这个日志出现的时候，我禁不住的留下了泪水-<em>-</em>-_-<br><code>21/09/12 14:53:39 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.1.0 using path:</code></p>
<h1 id="hosts配置"><a href="#hosts配置" class="headerlink" title="hosts配置"></a>hosts配置</h1><p>spark-sumit新增环境变量<code>HOSTS_FILE</code>,<br>HOSTS_FILE为<code>spark-config</code>中的pvc路径。<br>docker镜像的entrypoint中读取pvc中约定的host文件并追加到<code>/etc/hosts</code></p>
<h1 id="spark集成第三方数据源"><a href="#spark集成第三方数据源" class="headerlink" title="spark集成第三方数据源"></a>spark集成第三方数据源</h1><p>以下数据源可通过spark相关的jar包集成(放到<code>jars_ext</code>目录)，程序中配置连接参数就可以</p>
<ul>
<li>redis</li>
<li>elasticsearch</li>
<li>cassandra</li>
<li>jdbc:mysql/oracle</li>
</ul>
<h1 id="spark-submit示例"><a href="#spark-submit示例" class="headerlink" title="spark-submit示例"></a>spark-submit示例</h1><p>spark on k8s在dolphinscheduler的worker运行脚本<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line">BASEDIR=$(<span class="built_in">cd</span> `dirname <span class="variable">$0</span>`; <span class="built_in">pwd</span>)</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$BASEDIR</span></span><br><span class="line"><span class="built_in">source</span> /opt/dolphinscheduler/conf/env/dolphinscheduler_env.sh</span><br><span class="line"><span class="built_in">export</span> KUBECONFIG=/opt/dolphinscheduler/conf/config/kube_config.yaml</span><br><span class="line"><span class="variable">$&#123;SPARK3_HOME&#125;</span>/bin/spark-submit  \</span><br><span class="line">--master k8s://https://ip:6443  \</span><br><span class="line">--deploy-mode cluster  \</span><br><span class="line">--conf spark.kubernetes.namespace=spark  \</span><br><span class="line">--conf spark.kubernetes.container.image=bigdata/spark:0.1  \</span><br><span class="line">--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.SPARK_VERSION=3  \</span><br><span class="line">--conf spark.executorEnv.SPARK_VERSION=3  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars.path=file:///opt/cdh/lib/hive/lib/*.jar,file:///opt/cdh/lib/hadoop/client/*.jar  \</span><br><span class="line">--conf spark.sql.hive.metastore.jars=path  \</span><br><span class="line">--conf spark.sql.hive.metastore.version=1.1  \</span><br><span class="line"></span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-eventlog.options.claimName=spark-eventlog  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-eventlog.options.path=/spark/eventlog  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-eventlog.options.type=Directory  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-eventlog.mount.path=/spark/eventlog  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-eventlog.mount.readOnly=<span class="literal">false</span>  \</span><br><span class="line"></span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-eventlog.options.claimName=spark-eventlog  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-eventlog.options.path=/spark/eventlog  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-eventlog.options.type=Directory  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-eventlog.mount.path=/spark/eventlog  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-eventlog.mount.readOnly=<span class="literal">false</span>  \</span><br><span class="line">--conf spark.eventLog.dir=/spark/eventlog  \</span><br><span class="line">--conf spark.eventLog.enabled=<span class="literal">true</span>  \</span><br><span class="line">--conf spark.eventLog.compress=<span class="literal">true</span>  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-config.options.claimName=spark-config  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-config.options.path=/spark/config  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-config.options.type=Directory  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-config.mount.path=/spark/config  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-config.mount.readOnly=<span class="literal">true</span>  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-config.options.claimName=spark-config  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-config.options.path=/spark/config  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-config.options.type=Directory  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-config.mount.path=/spark/config  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-config.mount.readOnly=<span class="literal">true</span>  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.HADOOP_CONF_DIR=/spark/config/hadoop  \</span><br><span class="line">--conf spark.executorEnv.HADOOP_CONF_DIR=/spark/config/hadoop  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.PYSPARK_PYTHON=/spark/config/python-venv/bin/python  \</span><br><span class="line">--conf spark.executorEnv.PYSPARK_PYTHON=/spark/config/python-venv/bin/python  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.HOSTS=/spark/config/hosts  \</span><br><span class="line">--conf spark.executorEnv.HOSTS=/spark/config/hosts  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-application.options.claimName=spark-application  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-application.options.path=/spark/application  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-application.options.type=Directory  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-application.mount.path=/spark/application  \</span><br><span class="line">--conf spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-application.mount.readOnly=<span class="literal">false</span>  \</span><br><span class="line"></span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-application.options.claimName=spark-application  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-application.options.path=/spark/application  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-application.options.type=Directory  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-application.mount.path=/spark/application  \</span><br><span class="line">--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-application.mount.readOnly=<span class="literal">false</span>  \</span><br><span class="line"></span><br><span class="line">--conf spark.kubernetes.driverEnv.PRINCIPAL=hive  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.KRB5_CONFIG=/spark/application/2/1963/20954/37472/hive_krb5.conf  \</span><br><span class="line">--conf spark.kubernetes.driverEnv.KEYTAB=/spark/application/2/1963/20954/37472/hive.keytab  \</span><br><span class="line">--conf spark.executorEnv.PRINCIPAL=hive  \</span><br><span class="line">--conf spark.executorEnv.KRB5_CONFIG=/spark/application/2/1963/20954/37472/hive_krb5.conf  \</span><br><span class="line">--conf spark.executorEnv.KEYTAB=/spark/application/2/1963/20954/37472/hive.keytab  \</span><br><span class="line">--conf spark.kubernetes.kerberos.krb5.path=/spark/application/2/1963/20954/37472/hive_krb5.conf  \</span><br><span class="line">--conf spark.kerberos.principal=hive  \</span><br><span class="line">--conf spark.kerberos.keytab=/spark/application/2/1963/20954/37472/hive.keytab  \</span><br><span class="line">--conf spark.kubernetes.file.upload.path=s3a:/__SPARK_APP__  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.endpoint=http://ip:32030  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.access.key=xxx  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.secret.key=xxx  \</span><br><span class="line">--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem  \</span><br><span class="line">--conf spark.files.fetchTimeout=5m  \</span><br><span class="line">--conf spark.files.overwrite=<span class="literal">true</span>  \</span><br><span class="line">--conf spark.driver.memory=1024m  \</span><br><span class="line">--conf spark.executor.memory=1024m  \</span><br><span class="line">--conf spark.executor.instances=2  \</span><br><span class="line">--conf spark.kubernetes.executor.request.cores=1  \</span><br><span class="line">--conf spark.kubernetes.driver.request.cores=1  \</span><br><span class="line">--conf spark.kubernetes.driver.pod.name=spark-1-1  \</span><br><span class="line">--name spark-debug-demo  \</span><br><span class="line">--py-files <span class="built_in">local</span>:///spark/application/2/1963/20954/37472/spark/spark_on_k8s/spark_utils.zip   \</span><br><span class="line"><span class="built_in">local</span>:///spark/application/2/1963/20954/37472/spark/pyspark_demo.py</span><br></pre></td></tr></table></figure></p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://spark.apache.org/docs/3.1.2/sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore">spark3.1.2-interacting-with-different-versions-of-hive-metastore</a></li>
<li><a href="https://spark.apache.org/docs/2.4.8/sql-data-sources-hive-tables.html#interacting-with-different-versions-of-hive-metastore">spark2.4.8-interacting-with-different-versions-of-hive-metastore</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/setup-metastore-jars.html">hive-setup-metastore-jars</a></li>
<li><a href="https://spark.apache.org/docs/latest/hadoop-provided.html">spark-hadoop-provided</a></li>
<li><a href="https://archive.apache.org/dist/spark/spark-3.0.0/">spark-3.0-release</a></li>
<li><a href="https://spark.apache.org/docs/3.1.2/security.html#using-a-keytab">spark-using-a-keytab</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/dolphinScheduler/" rel="tag"># dolphinScheduler</a>
              <a href="/tags/k8s/" rel="tag"># k8s</a>
              <a href="/tags/cdh/" rel="tag"># cdh</a>
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/21/dolphinscheduler%E6%B5%81%E4%BB%BB%E5%8A%A1%E7%9B%91%E6%8E%A7/" rel="prev" title="dolphinscheduler流任务监控">
      <i class="fa fa-chevron-left"></i> dolphinscheduler流任务监控
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/21/%E5%A6%82%E4%BD%95%E8%AE%A9DolphinScheduler%E7%9A%84Spark%E4%BB%BB%E5%8A%A1%E6%94%AF%E6%8C%81K8S%E9%83%A8%E7%BD%B2/" rel="next" title="如何让DolphinScheduler的Spark任务支持K8S部署">
      如何让DolphinScheduler的Spark任务支持K8S部署 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#部署环境"><span class="nav-number">1.</span> <span class="nav-text">部署环境</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#部署要求"><span class="nav-number">1.1.</span> <span class="nav-text">部署要求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#验证环境"><span class="nav-number">1.2.</span> <span class="nav-text">验证环境</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#工作流程"><span class="nav-number">2.</span> <span class="nav-text">工作流程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#制作docker镜像"><span class="nav-number">3.</span> <span class="nav-text">制作docker镜像</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#cdh-base的镜像"><span class="nav-number">3.1.</span> <span class="nav-text">cdh-base的镜像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark镜像"><span class="nav-number">3.2.</span> <span class="nav-text">spark镜像</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#配置K8S环境"><span class="nav-number">4.</span> <span class="nav-text">配置K8S环境</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的PVC配置"><span class="nav-number">4.1.</span> <span class="nav-text">Spark的PVC配置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-pvc"><span class="nav-number">4.1.1.</span> <span class="nav-text">spark pvc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mount-glusterfs-pvc"><span class="nav-number">4.1.2.</span> <span class="nav-text">mount glusterfs pvc</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#采集executor日志"><span class="nav-number">5.</span> <span class="nav-text">采集executor日志</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#访问driver-ui"><span class="nav-number">6.</span> <span class="nav-text">访问driver ui</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#部署history-server"><span class="nav-number">7.</span> <span class="nav-text">部署history-server</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark数据存储"><span class="nav-number">8.</span> <span class="nav-text">spark数据存储</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#S3A"><span class="nav-number">8.1.</span> <span class="nav-text">S3A</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS"><span class="nav-number">8.2.</span> <span class="nav-text">HDFS</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kerberos认证"><span class="nav-number">9.</span> <span class="nav-text">kerberos认证</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#手动kinit"><span class="nav-number">9.1.</span> <span class="nav-text">手动kinit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DelegationTokenFetcher"><span class="nav-number">9.2.</span> <span class="nav-text">DelegationTokenFetcher</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#升级spark３"><span class="nav-number">9.3.</span> <span class="nav-text">升级spark３</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-hive版本兼容"><span class="nav-number">10.</span> <span class="nav-text">spark-hive版本兼容</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#测试路径"><span class="nav-number">10.1.</span> <span class="nav-text">测试路径</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#测试HiveClasspath"><span class="nav-number">10.2.</span> <span class="nav-text">测试HiveClasspath</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark2和spark3与hive集成区别"><span class="nav-number">10.3.</span> <span class="nav-text">spark2和spark3与hive集成区别</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#hosts配置"><span class="nav-number">11.</span> <span class="nav-text">hosts配置</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark集成第三方数据源"><span class="nav-number">12.</span> <span class="nav-text">spark集成第三方数据源</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-submit示例"><span class="nav-number">13.</span> <span class="nav-text">spark-submit示例</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number">14.</span> <span class="nav-text">参考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">geosmart</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">199</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">118</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/geosmart" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;geosmart" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">geosmart</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '8a29a27c357a64ed79fd',
      clientSecret: 'fe1b781c49fa9037803f3c4d3e94fbb677abac4d',
      repo        : 'geosmart.github.io',
      owner       : 'geosmart',
      admin       : ['geosmart'],
      id          : 'e52560902a497f343bf8c7345e16fc52',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
